# Unit 2 Notes | Statistics for Business II

## Intro to Probability

- The **probability** of an outcome is the proportion of times the outcome would occur if we observed the random process an infinite number of times.
- Probability is a **long-run proportion**, so it always lies between 0 and 1.

## Law of Large Numbers (LLN)

- Example: rolling a fair die many times.
  - Let $\hat{p}_n$ be the proportion of rolls that equal 1 after $n$ rolls.
  - As $n$ increases, $\hat{p}_n \rightarrow p = \frac{1}{6}$.
  
- The **Law of Large Numbers** states that as the number of observations increases, the sample proportion $\hat{p}_n$ of an outcome converges to the true probability $p$.

- In small samples, the sample proportion may appear to "defy" the Law of Large Numbers.
  - These deviations are normal due to randomness.
  - As the number of observations grows, the deviations become smaller and the proportion stabilizes around $p$.

```{r}
# Draw one value from 1 through 6
# This simulates rolling a fair six-sided die once
# 1:6 → possible outcomes (the faces of the die)
# size = 1 → number of rolls
# replace = TRUE → allows the same number to appear again in future rolls

sample(1:6, size = 1, replace = TRUE)

# True probability of rolling a 1 with a fair die
p <- 1/6

# Set seed once for reproducibility
set.seed(123)

# Create an empty list to store simulation results
z <- list()

# Repeat the experiment for n = 1 to 5000 rolls
for(i in 1:5000){
  
  # Simulate i die rolls and store results in a data frame
  # number of rolls (n)
  # rolls = outcomes from rolling a fair die i times
  z[[length(z)+1]] <- data.frame(
    n = i,
    rolls = sample(1:6, i, replace = TRUE)
  )
  
}

# Combine all stored data frames into one large data frame
z <- as.data.frame(do.call(rbind, z))

# Create indicator variable:
# 1 if roll equals 1, 0 otherwise
z$one <- ifelse(z$rolls == 1, 1, 0)

# View first few rows
head(z)

# For each value of n, compute the proportion of 1s
# This gives us the sample proportion p-hat_n
y <- aggregate(one ~ n, data = z, mean)

# View first few results
head(y)

# Plot sample proportion against number of rolls
par(mar = c(5, 5, 1, 1))
plot(y$n, y$one,
     xlab = "number of rolls (n)", 
     ylab = expression(hat(p)[n]),
     cex.lab = 1.5, 
     cex.axis = 1.25, 
     ylim = c(0, 2*p), 
     col = "darkblue", 
     pch = 19)
abline(h = p, lwd = 4, col = "plum")
legend("topright", 
       legend = c("True Value"),
       lwd = 4, col = "plum", 
       bty = "n", cex = 1.5)

```

## Disjoint (Mutually Exclusive) Outcomes

- Two outcomes are **disjoint** (or **mutually exclusive**) if they cannot occur at the same time.
  - When rolling a die, the outcomes 1 and 2 are disjoint — only one number can appear.
  - However, the outcomes 1 and “rolling an odd number” are **not** disjoint, since both occur when the roll is 1.

- If outcomes are disjoint, we can calculate the probability of either occurring by **adding their probabilities**.

$$
P(1 \text{ or } 2) = \frac{1}{6} + \frac{1}{6} = \frac{2}{6} = \frac{1}{3}
$$

### Addition Rule for Disjoint Outcomes

- If $A_1, A_2, \dots, A_k$ are disjoint events, then:

$$
P(A_1 \text{ or } A_2 \text{ or } \dots \text{ or } A_k)
= P(A_1) + P(A_2) + \dots + P(A_k)
$$
### Thinking in Terms of Events

- In practice, we often work with **events**, which are sets (collections) of outcomes.

- Let:
  - $A = \{1, 2\}$ (rolling a 1 or 2)
  - $B = \{4, 6\}$ (rolling a 4 or 6)

- Because $A$ and $B$ share no outcomes, they are **disjoint events**.

$$
P(A \text{ or } B) = P(A) + P(B)
= \frac{2}{6} + \frac{2}{6}
= \frac{4}{6}
= \frac{2}{3}
= 0.\bar{6}
$$

```{r}

set.seed(123)
z <- list()
for(i in 1:10000){
  z[[i]] <- sample(1:6, size = 1, replace = TRUE)
}

z <- unlist(z)
A <- ifelse(z == 1 | z == 2, 1, 0)
B <- ifelse(z == 4 | z == 6, 1, 0)

mean(A) + mean(B)

```

## When Events Are *Not* Disjoint

- Probability a randomly selected card is a diamond:

$$
P(\diamondsuit) = \frac{13}{52} = \frac{1}{4} = 0.25
$$

- Probability a randomly selected card is a face card:

$$
P(\text{face}) = \frac{12}{52} \approx 0.23
$$

- Probability a card is a **face card or a diamond**:

These events overlap (there are 3 face cards that are diamonds), so we must subtract the overlap:

$$
P(\text{face or } \diamondsuit)
= P(\text{face}) + P(\diamondsuit) - P(\text{face and } \diamondsuit)
$$

$$
= \frac{12}{52} + \frac{13}{52} - \frac{3}{52}
= \frac{22}{52}
\approx 0.42
$$

```{r}

z <- data.frame(
  number = rep(2:14, 4),
  suit = rep(c("d", "c", "s", "h"), 13)
)
head(z)

y <- list()
for(i in 1:10000){
  set.seed(i)
  z$order <- rnorm(n = 52)
  z <- z[order(z$order),]
  
  y[[length(y)+1]] <- data.frame(
    loop = i,
    true = ifelse(z[1,2] == "d" | z[1,1] %in% 11:13, 1, 0)
  )
}
y <- as.data.frame(do.call(rbind, y))
mean(y$true)


```

### General Addition Rule

- For **any** two events $A$ and $B$ (disjoint or not):

$$
P(A \text{ or } B)
= P(A) + P(B) - P(A \text{ and } B)
$$

- The term $P(A \text{ and } B)$ accounts for overlap.
- If $A$ and $B$ are disjoint, then:

$$
P(A \text{ and } B) = 0
$$

and the rule simplifies to simple addition.

## Probability Distribution

- A **probability distribution** lists all possible disjoint outcomes and their associated probabilities.

- For a valid probability distribution:
  - Outcomes must be **disjoint**.
  - Each probability must be between 0 and 1.
  - The probabilities must sum to **1**.

```{r}

set.seed(124)

x <- data.frame(
  dice1 = sample(1:6, 10000, replace = TRUE),
  dice2 = sample(1:6, 10000, replace = TRUE)
)

sum <- x$dice1 + x$dice2
t <- as.data.frame(table(sum))
head(t)

y <- data.frame(
  sum = t$sum,
  prob = t$Freq / sum(t$Freq)
)

par(mar = c(5, 5, 1, 1))
barplot(height = y$prob, names = y$sum,
        ylim = c(0, 0.2),
        xlab = "Dice Sum", ylab = "Probability",
        cex.axis = 1.25, cex.lab = 1.5, cex.names = 1.25,
        col = "darkblue")

```

## Independence

- Two processes (or events) are **independent** if knowing the outcome of one provides no information about the outcome of the other.
  - Example: Flipping a coin and rolling a die are independent — knowing the coin landed heads tells us nothing about the die roll.
  - In contrast, stock prices often move together, so their movements are typically **not** independent.

### Multiplication Rule for Independent Events

- If $A$ and $B$ are independent events, then the probability that **both** occur is:

$$
P(A \text{ and } B) = P(A) \times P(B)
$$

- More generally, if $A_1, A_2, \dots, A_k$ are independent events, then:

$$
P(A_1 \text{ and } A_2 \text{ and } \dots \text{ and } A_k)
= P(A_1) \times P(A_2) \times \dots \times P(A_k)
$$

### Example: Rolling Two Dice

- What is the probability of rolling two 1s?

$$
P(1 \text{ and } 1)
= \frac{1}{6} \times \frac{1}{6}
= \frac{1}{36}
= 0.02\bar{7}
$$

```{r}

x$ones <- ifelse(x$dice1 == 1 & x$dice2 == 1, 1, 0)
mean(x$ones)

```

## Marginal and Joint Probabilities

- **Marginal probabilities** are the probabilities based on a single variable without regard to any other variables

- A probability of outcomes for two or more variables or processes is called a **joint probability**

### Example: Photo Classification

- Data scientists are developing a classifier to determine whether a photo is related to fashion.
  - The `photo_classify` dataset contains 1,822 photos from a photo-sharing website

- Each photo receives **two classifications**:
  - `mach_learn`: the binary prediction from a machine learning model
  - `truth`: a binary human-reviewed classification

- We treat the human classification (`truth`) as the **ground truth** when evaluating model performance.

```{r}

x <- read.csv("data/photo_classify.csv")

table(x$mach_learn)
table(x$truth)

z <- as.data.frame.matrix(table(predicted = x$mach_learn, actual = x$truth))
z$total <- rowSums(z)
z <- rbind(z, total = colSums(z))
z

```

- What is the probability that the machine algorithm will classify a picture as being about fashion?
  - $\dfrac{219}{1822} \approx 12\%$
  
```{r}

mean(ifelse(x$mach_learn == "pred_fashion", 1, 0))

```

- What is the probability that:
  - the machine algorithm will classify a picture as being about fashion *and* 
  - what is the probability that picture is about fashion?
    - $P(\hat{Fashion} \text{ and } Fashion) = \dfrac{197}{1822} \approx 10.8\%$
    
```{r}

mean(ifelse(x$mach_learn == "pred_fashion" & x$truth == "fashion", 1, 0))

```

## Conditional Probability

- **Conditional probability** measures the probability that an event occurs **given that** another event is known to have occurred.

- A conditional probability has two parts:
  - The event of interest ($A$)
  - The condition ($B$)

- Think of the condition as information we already know to be true.

- Notation:

$$
P(A \mid B)
$$

- Read as: "the probability that $A$ occurs given that $B$ has occurred."

### Example: Photo Classification

- Conditional upon the machine learning prediction that the picture is about fashion, what is the probability that the picture is truly about fashion?
  - $P(Fashion|\hat{Fashion}) = \dfrac{197}{219} \approx 90\%$
  
```{r}

x_predf <- x[x$mach_learn == "pred_fashion",]
mean(ifelse(x_predf$truth == "fashion", 1, 0))

```

- Conditional upon the picture is truly being about fashion, what is the probability that the machine learning estimation predicted that the picture is about fashion?
  - $P(\hat{Fashion}|Fashion) = \dfrac{197}{309} \approx 63.8\%$
  
 Takeaways: this measures **model precision**
  - When the algorithm predicts that a picture is about fashion, it is correct about 90% of the time
  - In practice, this means that the model’s positive predictions are usually trustworthy
  - False positives are relatively rare
  
```{r}

x_f <- x[x$truth == "fashion",]
mean(ifelse(x_f$mach_learn == "pred_fashion", 1, 0))

```

- Takeaways: this measures **model sensitivity**
  - Out of all the pictures that are truly about fashion, the model successfully identifies about 64% of them
  - In practice, this means the model misses a fair number of true fashion images
  - False negatives are fairly common
  
- The model is **conservative**: when it predicts "fashion," it is usually correct (high precision).

- However, it is **incomplete**: it fails to identify a substantial share of true fashion images (moderate sensitivity).

- In short, the algorithm is strong at being right when it predicts fashion, but weaker at capturing all fashion-related content.

### Equations: Conditional Probability

$$
P(A|B) = \dfrac{P(A \text{ and } B)}{P(B)}
$$

- Note that we can rewrite this equation using algebra:

$$
P(A \text{ and } B) = P(A|B) \times P(B)
$$  

$$
P(Fashion|\hat{Fashion}) = \dfrac{P(Fashion \text{ and } \hat{Fashion})}{P(\hat{Fashion})} = \dfrac{0.108}{0.120} \approx 90\%
$$

### Example: Smallpox Vaccine

- The smallpox data set provides a sample of 6,224 individuals from the year 1721 who were exposed to smallpox in Boston
  - Doctors hypothesized that exposing a person to the disease in a controlled form could reduce the likelihood of death


```{r}

x <- read.csv("data/smallpox.csv")

z <- as.data.frame.matrix(table(vaccinated = x$inoculated, lived = x$result))
z$total <- rowSums(z)
z <- rbind(z, total = colSums(z))
z

# percentages
round(z / z[3,3] * 100, 1)

```

- What is the the probability that a randomly selected person who was (not) inoculated died from smallpox?
  - $P(Died|Vaccinated) = \dfrac{0.001}{0.039} \approx 2.5\%$
  - $P(Died|Not) = \dfrac{0.136}{0.961} \approx 14.1\%$

```{r}

x_vax <- x[x$inoculated == "yes",]
mean(ifelse(x_vax$result == "died", 1, 0))

x_n_vax <- x[x$inoculated == "no",]
mean(ifelse(x_n_vax$result == "died", 1, 0))

```

- What is probability that a resident was not inoculated and lived?
  - $P(\text{Lived and Not}) = P(Lived|Not) \times P(Not) = \dfrac{5136}{5980} \times \dfrac{5980}{6224} \approx 82.5\%$
  
```{r}

mean(ifelse(x_n_vax$result == "lived", 1, 0))

```

- What is probability that a resident was not inoculated and lived?
  - $P(\text{Lived and Not}) = P(Lived|Not) \times P(Not) = \dfrac{5136}{5980} \times \dfrac{5980}{6224} \approx 82.5\%$
  - $P(\text{Lived and Not}) = P(Not|Lived) \times P(Lived) = \dfrac{5136}{5374} \times \dfrac{5374}{6224} \approx 82.5\%$
  
```{r}

mean(ifelse(x$result == "lived" & x$inoculated == "no", 1, 0))

```

### Barchart: Smallpox Vaccine

```{r}

x$lived <- ifelse(x$result == "lived", 1, 0)
x$vaxxed <- ifelse(x$inoculated == "yes", 1, 0)

y <- aggregate(lived ~ vaxxed, x, function(x) c(mean = mean(x),
                                                sd = sd(x),
                                                n = length(x)))
y <- as.data.frame(do.call(cbind, y))
y$min <- y$mean - 1.96 * y$sd / sqrt(y$n)
y$max <- y$mean + 1.96 * y$sd / sqrt(y$n)

par(mar = c(5, 5, 1, 1))
bp <- barplot(
  height = y$mean,
  names.arg = c("No Vacc.", "Vaccinated"),
  ylim = c(0, 1),
  ylab = "Survival Rate | Vaccination Status",
  xlab = "Vaccination Status",
  cex.lab = 1.5, cex.axis = 1.25, cex.names = 1.25,
  col = "darkblue"
)
arrows(x0 = bp, y0 = y$min, x1 = bp, y1 = y$max,
       code = 3, angle = 90, length = 0.05, lwd = 4,
       col = "darkgrey")

```
